{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SgeabwCbFx-9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TD3 algo for \"bipedal-walker-v2\" \n",
        "# https://gym.openai.com/envs/BipedalWalker-v2/"
      ]
    },
    {
      "metadata": {
        "id": "nzgnmKXrCS1s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Это реализация алгоритма TD3. \\\n",
        "Как до него дошёл: \\\n",
        "1) Увидел задание, решил уже известным алгоритмом Augmented Random Search. Итог 300+ за ~1000 итераций. Это хоршо, но в задании алгоритм автор--критик. Незаметил( \\\n",
        "2) Алогитм DDPG выдал >100 за 500 шагов, резкий пик в 202 за 642 шагов, затем упал к +20, и к 1500 шагам оттуда не вылез. \\\n",
        "3) Пошел гуглить альтернативные алгоритмы. Нашел список actor-critic model, поподробнее прочитал про DDPG, нашел описание его минусов(один из них, возможно, и привел к быстрому проседанию до скора 20(он таковым остался и через 700 итераций +-), после резкого пика в 202. Это ситуаия, когда Q-function начинает переоценивать какое-нибудь Q-value, что приводит к искажению policy). В качестве их решения был предложен TD3. \\\n",
        "Подробнее -- в конспекте-выдежке из нижних статей:\n",
        "https://drive.google.com/drive/folders/1NIasAAWASQdg8xAk4xz4qo4M1-PCHhOa?usp=sharing\n",
        "\n",
        "Статьи: \\\n",
        "[ARS] --https://arxiv.org/pdf/1803.07055.pdf \\\n",
        "[DDPG] --https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background \\\n",
        "[TD3] -- https://spinningup.openai.com/en/latest/algorithms/td3.html\n"
      ]
    },
    {
      "metadata": {
        "id": "xewZRGQ4Ky9E",
        "colab_type": "code",
        "outputId": "8625df10-ea2d-4e67-d3f5-3f134f9ad893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!apt-get update\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!apt-get install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Hit:1 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [83.2 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [278 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [130 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [753 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [593 kB]\n",
            "Fetched 2,001 kB in 1s (1,428 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2afEz48NDk5",
        "colab_type": "code",
        "outputId": "15afe2a8-1e99-4b6d-bfdd-d8a60d8576eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d box2d-kengz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 83%\r\rReading package lists... 83%\r\rReading package lists... 83%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 97%\r\rReading package lists... 97%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied: box2d-kengz in /usr/local/lib/python3.6/dist-packages (2.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WvyXMeUqNDnM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Start virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sBh4WG1zOlhN",
        "colab_type": "code",
        "outputId": "21cf0663-7b41-4653-8577-85e75ad7786c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ymlCjKDANDp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-7KN715ND7U",
        "colab_type": "code",
        "outputId": "62a3614c-39d7-41c1-af63-dcd694c00d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8z-YuLNkC307",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WPkUOhWtND9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is implementation of the set D, discribed in the paper, \n",
        "# and in the my notes.\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self):\n",
        "        self.buffer = []\n",
        "    \n",
        "    def add(self, transition):\n",
        "        #(state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    # on some step of TD3, I should chose random item from set.\n",
        "    # Why random?\n",
        "    # Because optional Q-func satisfy Bellman equation for all posible transitions\n",
        "    # more details -- in my notes, page2\n",
        "    def sample(self, batch_size):\n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hit1RbQYNKv-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MyActor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(MyActor, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            torch.nn.Linear(state_dim, 400),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(400, 300),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(300,action_dim)\n",
        "        )\n",
        "        self.max_action = max_action\n",
        "    def forward(self, state):\n",
        "        output = torch.tanh(self.main(state)) * self.max_action\n",
        "        return output\n",
        "      \n",
        "      \n",
        "class MyCritic(nn.Module):    \n",
        "    def __init__(self,state_dim, action_dim):\n",
        "        super(MyCritic, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            torch.nn.Linear(state_dim + action_dim, 400),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(400, 300),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(300,1)\n",
        "        )\n",
        "    def forward(self, state, action):\n",
        "        output = self.main(torch.cat([state, action], 1))\n",
        "        return output\n",
        "    \n",
        "# WHY and how does it work -- in my notes \n",
        "class TD3:\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        \n",
        "        self.actor = MyActor(state_dim, action_dim, max_action).float().to(device)\n",
        "        self.actor_target = MyActor(state_dim, action_dim, max_action).float().to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters())\n",
        "        \n",
        "        self.critic_1 = MyCritic(state_dim, action_dim).float().to(device)\n",
        "        self.critic_1_target = MyCritic(state_dim, action_dim).float().to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters())\n",
        "        \n",
        "        self.critic_2 = MyCritic(state_dim, action_dim).float().to(device)\n",
        "        self.critic_2_target = MyCritic(state_dim, action_dim).float().to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters())\n",
        "        \n",
        "        self.max_action = max_action\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).float().to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "    \n",
        "    def update(self, replay_buffer, n_iter, batch_size, gamma, p, policy_noise, noise_clip, policy_delay):\n",
        "        \n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(state).float().to(device)\n",
        "            action = torch.FloatTensor(action_).float().to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).float().to(device)\n",
        "            next_state = torch.FloatTensor(next_state).float().to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).float().to(device)\n",
        "            \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).float().to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "                \n",
        "                # p, (\"poliyac\" in the article) updating\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (p * target_param.data) + ((1-p) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (p * target_param.data) + ((1-p) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (p * target_param.data) + ((1-p) * param.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__jMx92djTT7",
        "colab_type": "code",
        "outputId": "67564778-e7f4-49b0-8364-3a53a41ebc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1664
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train():\n",
        "    ######### Hyperparameters #########\n",
        "    env_name = \"BipedalWalker-v2\"\n",
        "    log_interval = 10           # print avg reward after interval\n",
        "    random_seed = 0\n",
        "    gamma = 0.99                # discount for future rewards\n",
        "    batch_size = 100            # num of transitions sampled from replay buffer\n",
        "    exploration_noise = 0.1 \n",
        "    p = 0.995              # target policy update parameter (1-tau)\n",
        "    policy_noise = 0.2          # target policy smoothing noise\n",
        "    noise_clip = 0.5\n",
        "    policy_delay = 2            # delayed policy updates parameter\n",
        "    max_episodes = 10000         # max num of episodes\n",
        "    max_timesteps = 2000        # max timesteps in one episode\n",
        "    directory = \"./preTrained/{}\".format(env_name) # save trained models\n",
        "    filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
        "    ###################################\n",
        "    \n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    \n",
        "    policy = TD3(state_dim, action_dim, max_action)\n",
        "    replay_buffer = ReplayBuffer()\n",
        "    \n",
        "    if random_seed:\n",
        "        print(\"Random Seed: {}\".format(random_seed))\n",
        "        env.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "    \n",
        "    # logging variables:\n",
        "    avg_reward = 0\n",
        "    ep_reward = 0\n",
        "    log_f = open(\"log.txt\",\"w+\")\n",
        "    \n",
        "    # training procedure:\n",
        "    for episode in range(1, max_episodes+1):\n",
        "        state = env.reset()\n",
        "        for t in range(max_timesteps):\n",
        "            # select action and add exploration noise:\n",
        "            action = policy.select_action(state)\n",
        "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
        "            action = action.clip(env.action_space.low, env.action_space.high)\n",
        "            \n",
        "            # take action in env:\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            state = next_state\n",
        "            \n",
        "            avg_reward += reward\n",
        "            ep_reward += reward\n",
        "            \n",
        "            # if episode is done then update policy:\n",
        "            if done or t==(max_timesteps-1):\n",
        "                policy.update(replay_buffer, t, batch_size, gamma, p, policy_noise, noise_clip, policy_delay)\n",
        "                break\n",
        "#         break\n",
        "                \n",
        "        if (ep_reward) >= 300:\n",
        "            print(\"====FINALLY! GOT IT!=====\")\n",
        "            print(\"At episode: {}\".format(episode))\n",
        "            log_f.close()\n",
        "            break\n",
        "            \n",
        "        ep_reward = 0\n",
        "        \n",
        "        if episode % log_interval == 0:\n",
        "            avg_reward = int(avg_reward / log_interval)\n",
        "            print(\"Episode: {}\\tAverage Reward: {}\".format(episode, avg_reward))\n",
        "            avg_reward = 0\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "Episode: 10\tAverage Reward: -110\n",
            "Episode: 20\tAverage Reward: -107\n",
            "Episode: 30\tAverage Reward: -106\n",
            "Episode: 40\tAverage Reward: -127\n",
            "Episode: 50\tAverage Reward: -112\n",
            "Episode: 60\tAverage Reward: -103\n",
            "Episode: 70\tAverage Reward: -110\n",
            "Episode: 80\tAverage Reward: -118\n",
            "Episode: 90\tAverage Reward: -113\n",
            "Episode: 100\tAverage Reward: -110\n",
            "Episode: 110\tAverage Reward: -92\n",
            "Episode: 120\tAverage Reward: -106\n",
            "Episode: 130\tAverage Reward: -124\n",
            "Episode: 140\tAverage Reward: -139\n",
            "Episode: 150\tAverage Reward: -136\n",
            "Episode: 160\tAverage Reward: -94\n",
            "Episode: 170\tAverage Reward: -83\n",
            "Episode: 180\tAverage Reward: -93\n",
            "Episode: 190\tAverage Reward: -90\n",
            "Episode: 200\tAverage Reward: -83\n",
            "Episode: 210\tAverage Reward: -84\n",
            "Episode: 220\tAverage Reward: -53\n",
            "Episode: 230\tAverage Reward: 19\n",
            "Episode: 240\tAverage Reward: 28\n",
            "Episode: 250\tAverage Reward: 52\n",
            "Episode: 260\tAverage Reward: 199\n",
            "Episode: 270\tAverage Reward: 245\n",
            "Episode: 280\tAverage Reward: 168\n",
            "Episode: 290\tAverage Reward: 211\n",
            "Episode: 300\tAverage Reward: 224\n",
            "Episode: 310\tAverage Reward: -108\n",
            "Episode: 320\tAverage Reward: 153\n",
            "Episode: 330\tAverage Reward: 180\n",
            "Episode: 340\tAverage Reward: 172\n",
            "Episode: 350\tAverage Reward: 247\n",
            "Episode: 360\tAverage Reward: 137\n",
            "Episode: 370\tAverage Reward: 225\n",
            "Episode: 380\tAverage Reward: 193\n",
            "Episode: 390\tAverage Reward: 134\n",
            "Episode: 400\tAverage Reward: 274\n",
            "Episode: 410\tAverage Reward: 255\n",
            "Episode: 420\tAverage Reward: 175\n",
            "Episode: 430\tAverage Reward: 276\n",
            "Episode: 440\tAverage Reward: 282\n",
            "Episode: 450\tAverage Reward: 282\n",
            "Episode: 460\tAverage Reward: 283\n",
            "Episode: 470\tAverage Reward: 282\n",
            "Episode: 480\tAverage Reward: 286\n",
            "Episode: 490\tAverage Reward: 253\n",
            "Episode: 500\tAverage Reward: 282\n",
            "Episode: 510\tAverage Reward: 287\n",
            "Episode: 520\tAverage Reward: 289\n",
            "Episode: 530\tAverage Reward: 288\n",
            "Episode: 540\tAverage Reward: 291\n",
            "Episode: 550\tAverage Reward: 290\n",
            "Episode: 560\tAverage Reward: 290\n",
            "Episode: 570\tAverage Reward: 290\n",
            "Episode: 580\tAverage Reward: 290\n",
            "Episode: 590\tAverage Reward: 276\n",
            "Episode: 600\tAverage Reward: 257\n",
            "Episode: 610\tAverage Reward: 290\n",
            "Episode: 620\tAverage Reward: 292\n",
            "Episode: 630\tAverage Reward: 292\n",
            "Episode: 640\tAverage Reward: 262\n",
            "Episode: 650\tAverage Reward: 294\n",
            "Episode: 660\tAverage Reward: 279\n",
            "Episode: 670\tAverage Reward: 294\n",
            "Episode: 680\tAverage Reward: 294\n",
            "Episode: 690\tAverage Reward: 293\n",
            "Episode: 700\tAverage Reward: 275\n",
            "Episode: 710\tAverage Reward: 292\n",
            "Episode: 720\tAverage Reward: 257\n",
            "Episode: 730\tAverage Reward: 293\n",
            "Episode: 740\tAverage Reward: 292\n",
            "Episode: 750\tAverage Reward: 294\n",
            "Episode: 760\tAverage Reward: 267\n",
            "Episode: 770\tAverage Reward: 294\n",
            "Episode: 780\tAverage Reward: 291\n",
            "Episode: 790\tAverage Reward: 295\n",
            "Episode: 800\tAverage Reward: 293\n",
            "Episode: 810\tAverage Reward: 296\n",
            "====FINALLY! GOT IT!=====\n",
            "At episode: 811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gn9xF6-Iju-H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}